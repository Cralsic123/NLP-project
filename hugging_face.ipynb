{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZh_2KOpoA3r"
      },
      "outputs": [],
      "source": [
        "pip install transformers pandas numpy scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv('ag_news_train.csv', header=None)\n",
        "test_df = pd.read_csv('ag_news_test.csv', header=None)\n",
        "\n",
        "# Preprocess the text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "train_df[1] = train_df[1].apply(lambda x: ' '.join([word.lower() for word in word_tokenize(x) if word.lower() not in stop_words and word not in string.punctuation]))\n",
        "test_df[1] = test_df[1].apply(lambda x: ' '.join([word.lower() for word in word_tokenize(x) if word.lower() not in stop_words and word not in string.punctuation]))\n",
        "\n",
        "# Convert the labels to integers\n",
        "train_df[0] = train_df[0] - 1\n",
        "test_df[0] = test_df[0] - 1\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "train_data = train_df[1].values\n",
        "train_labels = train_df[0].values\n",
        "test_data = test_df[1].values\n",
        "test_labels = test_df[0].values\n"
      ],
      "metadata": {
        "id": "CbbwvbHMostD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(train_data.tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_data.tolist(), truncation=True, padding=True)\n",
        "\n",
        "# Convert the data to tensorflow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Fine-tune the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
        "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\n"
      ],
      "metadata": {
        "id": "q7uMswARovIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred = np.argmax(model.predict(test_dataset.batch(16)).logits, axis=1)\n",
        "accuracy = accuracy_score(test_labels, y_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred, average='weighted')\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1-score:', f1)\n"
      ],
      "metadata": {
        "id": "H-zl6v21oz4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "test_text = [\"Scientists have discovered a new planet that could support life.\", \n",
        "             \"The stock market is booming as companies report record profits.\", \n",
        "             \"The World Cup soccer tournament will be held in Qatar next year.\"]\n",
        "test_encodings = tokenizer(test_text, truncation=True, padding=True)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings)))\n",
        "y_pred = np.argmax(model.predict(test_dataset.batch(1)).logits, axis=1)\n",
        "\n",
        "# Print the predictions\n",
        "for text, label in zip(test_text, y_pred):\n",
        "    print(f'{text} => {label}')\n"
      ],
      "metadata": {
        "id": "fv3kG0y3o0sJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}